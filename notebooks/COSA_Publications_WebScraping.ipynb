{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Publications_WebScraping",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "xtltjaRbVyZK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## WebScraping on the https://thecosa.org/ (COSA: COMMUTTEE ON SUSTAINALBILITY ASSESSMENT)"
      ],
      "metadata": {
        "id": "ghbK7bJFVIZ5"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "from bs4 import BeautifulSoup"
      ],
      "metadata": {
        "id": "3vsJjNI3SiG1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hAclBLa8Sa2a"
      },
      "outputs": [],
      "source": [
        "# Identify the target website's address, i.e., URL\n",
        "url = 'https://thecosa.org/publications/'\n",
        "# Add an user agent to aviod 403 ERROR\n",
        "user_agent = \"Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/79.0.3945.88 Safari/537.37\"\n",
        "# Create a response object to get the web page's HTML content\n",
        "get_url = requests.get(url, headers={'User-Agent': user_agent})\n",
        "# Create a beautiful soup object to parse HTML text with the help of the html.parser\n",
        "soup = BeautifulSoup(get_url.text, 'html.parser')\n",
        "# Check website's response\n",
        "print(get_url)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this case, without user agent, get_url will lead to 403 ERROR. Please refer to answer to the question poseted on [stackoverflow](https://stackoverflow.com/questions/60694611/web-scraping-error-http-error-403-forbidden)"
      ],
      "metadata": {
        "id": "7IQN_vAXWV9r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use prettify() method to make the HTML be nicely formatted\n",
        "# This line of code is optional, which is used to take a look at HTML\n",
        "# print(soup.prettify()) "
      ],
      "metadata": {
        "id": "8gG2B4ilVKDQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creat a list to store data\n",
        "article_list = []\n",
        "# Find all the topics\n",
        "topics = soup.find_all('div',{'class':'fusion-post-content post-content'})\n",
        "# Calculate the number of topics\n",
        "topic_count = 0\n",
        "# Calculate the number of pdf links\n",
        "pdf_count = 0\n",
        "\n",
        "for item in topics:\n",
        "  topic_count += 1\n",
        "  # Fetch title of the article\n",
        "  title = item.find('h2').find('a').text\n",
        "  # Fetch the pdf link\n",
        "  try:\n",
        "    download_link = item.find('div',{'class':'fusion-alignleft'}).find('a').get('href')   \n",
        "  except:\n",
        "    pass \n",
        "  # Get effective link\n",
        "  if download_link.endswith('pdf') and download_link.startswith('http') :  \n",
        "    ready_to_download = 'Y'  \n",
        "    pdf_count += 1\n",
        "  else:\n",
        "    ready_to_download = 'N'\n",
        "  # Creat a dict to stor article info\n",
        "  article = {'title': title,\n",
        "             'ready_to_download': ready_to_download,\n",
        "             'pdf_link': download_link}\n",
        "  \n",
        "  article_list.append(article)\n",
        "print(f'There are a total of {topic_count} posted on the website, {pdf_count} of which have effective pdf link') "
      ],
      "metadata": {
        "id": "GVA4nXQxXJjx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pulications = pd.DataFrame(article_list)\n",
        "pulications"
      ],
      "metadata": {
        "id": "ZQjmtsEnjg1b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pulications.to_csv('publications.csv',index=False)"
      ],
      "metadata": {
        "id": "GSXIA-Bxiph9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}